import pandas as pd
import numpy as np
data = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/User_registered_5_courses.csv') 
data.head()

data2 = data.drop(['Unnamed: 0','Unnamed: 0.1','age_group','country','state','college','local_chapter','graduation year','profession','department','study_year'], axis = 1)
data2.head()

data3 = data2.drop(['qualification','designation','motivation', 'exam_taker', 'Course_id_y'], axis = 1)
data3.head()

DataGrouped = data3.groupby(['user_id', 'noc19-ae01']).sum().reset_index()
DataGrouped


def create_DataBinary(DataGrouped):
  DataBinary = DataGrouped.copy()
  DataBinary['True'] = 1
  return DataBinary

DataBinary = create_DataBinary(DataGrouped)
DataBinary.head()

purchase_data[‘SalesItem’] = ‘I’ + 
purchase_data[‘SalesItem’].astype(str)


from scipy.sparse import coo_matrix, csr_matrix
from sklearn.metrics.pairwise import cosine_similarity
from sklearn.preprocessing import LabelEncoder

def GetItemItemSim(user_ids, product_ids):
SalesItemCustomerMatrix = csr_matrix(([1]*len(user_ids), (product_ids, user_ids)))
similarity = cosine_similarity(SalesItemCustomerMatrix)
return similarity, SalesItemCustomerMatrix




import pandas as pd
import numpy as np
import plotly.express as px
import plotly.graph_objects as go
import networkx as nx
from mlxtend.preprocessing import TransactionEncoder
from mlxtend.frequent_patterns import apriori,association_rules
import matplotlib.pyplot as plt
plt.style.use('default')
data = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/Five_Courses_updated.csv') 
data.shape

data.head()

data.describe()

import random
p = 0.005
df = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/Five_Courses_updated.csv', header=0, skiprows=lambda i: i>0 and random.random()>p)
df

df['noc19-ae01']

selection1 = []
for i in range(0, df.shape[0]):
    for j in range(0, df.shape[1]):
        selection1.append(df.values[i,j])
        
selection1 = np.array(selection1)
# 2. Transform Them a Pandas DataFrame
df = pd.DataFrame(selection1, columns=["courses"]) 
df["incident_count"] = 1 # Put 1 to Each Item For Making Countable Table, to be able to perform Group By

# 3. Delete NaN courses from Dataset
indexNames = df[df['courses'] == "nan" ].index
df.drop(indexNames , inplace=True)

# 4. Final Step: Make a New Appropriate Pandas DataFrame for Visualizations  
df_table = df.groupby("courses").sum().sort_values("incident_count", ascending=False).reset_index()

# 5. Initial Visualizations
df_table.head(10).style.background_gradient(cmap='Blues')
